{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 设置HTTP和HTTPS代理\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Qwen2ForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('qwen/Qwen2.5-Coder-1.5B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/home/linzhisheng/.cache/modelscope/hub/qwen/Qwen2___5-Coder-1___5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class multiHeadAttention:\n",
    "    def __init__(self, hidden_dim, head_num, group_num) -> None:\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = self.hidden_dim // self.head_num\n",
    "        self.group_num = group_num\n",
    "        self.key_value_head_num = self.head_num // group_num\n",
    "        \n",
    "        self.q = torch.nn.Linear(hidden_dim, self.head_num * self.head_dim)\n",
    "        self.k = torch.nn.Linear(hidden_dim, self.key_value_head_num * self.head_dim)\n",
    "        self.v = torch.nn.Linear(hidden_dim, self.key_value_head_num * self.head_dim)\n",
    "        self.o = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bz, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        query = self.q(x).view(bz, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        key = self.k(x).view(bz, seq_len, self.key_value_head_num, self.head_dim).transpose(1, 2)\n",
    "        value = self.v(x).view(bz, seq_len, self.key_value_head_num, self.head_dim).transpose(1, 2)\n",
    "        # output = self.o(x).view(bz, seq_len, self.head_num, self.head_dim)\n",
    "        \n",
    "        key = torch.repeat_interleave(key, dim=1, repeats=self.group_num)\n",
    "        value = torch.repeat_interleave(value, dim=1, repeats=self.group_num)\n",
    "        print(value.shape)\n",
    "        \n",
    "        # print(query.shape)\n",
    "        score = torch.matmul(query, key.transpose(2,3)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        prob = self.softmax(score)\n",
    "        prob = self.dropout(prob)\n",
    "        \n",
    "        output = torch.matmul(prob, value).transpose(1,2).contiguous().view(bz, seq_len, hidden_dim)\n",
    "        \n",
    "        output = self.o(output)\n",
    "        return output\n",
    "\n",
    "x = torch.randn(12, 24, 768)\n",
    "attention = multiHeadAttention(768, 2, 2)\n",
    "\n",
    "output = attention.forward(x)\n",
    "\n",
    "output.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理spider数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.common_utils import read_json, write_json\n",
    "from utils.schema_utils import scm_dict2text \n",
    "from utils.prompt_utils import nl2sqlte_template, gen_train_prompt\n",
    "\n",
    "data = read_json('spider/train_spider.json')\n",
    "\n",
    "table = read_json('spider/tables.json')\n",
    "schema_map = {}\n",
    "\n",
    "sql_template = \"\"\"【DB_ID】 {db_id}\n",
    "【Schema】\n",
    "{tables_info}\n",
    "【Foreign keys】\n",
    "{fks_info}\"\"\"\n",
    "\n",
    "for schema_dict in table:\n",
    "    \n",
    "    db_id = schema_dict['db_id']\n",
    "    \n",
    "    tables_info = ''\n",
    "    fks_info = ''\n",
    "    \n",
    "    column_names_original = schema_dict['column_names_original']\n",
    "    table_names = schema_dict['table_names_original']\n",
    "    column_types = schema_dict['column_types']\n",
    "    foreign_keys = schema_dict['foreign_keys']\n",
    "    \n",
    "    column_infos_map = {}\n",
    "    \n",
    "    for forengn_key in foreign_keys:\n",
    "        left = forengn_key[0]\n",
    "        right = forengn_key[1]\n",
    "        \n",
    "        left_info = column_names_original[left]\n",
    "        left_table = table_names[left_info[0]]\n",
    "        left_name = left_info[1]\n",
    "        right_info = column_names_original[right]\n",
    "        right_table = table_names[right_info[0]]\n",
    "        right_name = right_info[1]\n",
    "        \n",
    "        fks_info += f'{left_table}.{left_name}={right_table}.{right_name}\\n'        \n",
    "    \n",
    "    \n",
    "    for idx, column_info in enumerate(column_names_original):\n",
    "        table_idx = column_info[0]\n",
    "        column_name = column_info[1]\n",
    "        column_type = column_types[idx-1]\n",
    "        if table_idx == -1:\n",
    "            continue\n",
    "        if table_names[table_idx] not in column_infos_map:\n",
    "            column_infos_map[table_names[table_idx]] = {'column_name':[], 'column_type':[]}\n",
    "        column_infos_map[table_names[table_idx]]['column_name'].append(column_name)\n",
    "        column_infos_map[table_names[table_idx]]['column_type'].append(column_type)\n",
    "    \n",
    "    # print(column_infos_map)\n",
    "    \n",
    "    \n",
    "    for i, table_name in enumerate(table_names):\n",
    "        t = f'# Table: {table_name}\\n[\\n'\n",
    "        column_infos = column_infos_map[table_name]\n",
    "        \n",
    "        for j, column_name in enumerate(column_infos['column_name']):\n",
    "            t += f'  ({column_name}:{column_infos[\"column_type\"][j]}),\\n'\n",
    "        \n",
    "        t += ']'\n",
    "        \n",
    "        tables_info += t + '\\n'\n",
    "    \n",
    "    schema = sql_template.format(db_id=db_id, tables_info=tables_info, fks_info=fks_info)\n",
    "    schema_map[schema_dict['db_id']] = schema\n",
    "\n",
    "# print(schema_map)\n",
    "\n",
    "\n",
    "result = []\n",
    "for idx, row in enumerate(data):\n",
    "    question = row['question']\n",
    "    db_id = row['db_id']\n",
    "    sql = row['query']\n",
    "    schema = schema_map[db_id]\n",
    "    # result.append({'question':question, 'schema':schema, 'sql':sql, 'db_id':db_id})\n",
    "    data_item = {'question':question, 'db_schema':schema, 'sql':sql, 'evidence': ''}\n",
    "\n",
    "    prompt = gen_train_prompt(idx, data_item, 'sqlite')\n",
    "    prompt['db_name'] = db_id\n",
    "    result.append(prompt)\n",
    "\n",
    "\n",
    "\n",
    "write_json('spider/train_spider_chat.json', result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【DB_ID】 concert_singer\n",
      "【Schema】\n",
      "# Table: stadium\n",
      "[\n",
      "  (Stadium_ID:text),\n",
      "  (Location:number),\n",
      "  (Name:text),\n",
      "  (Capacity:text),\n",
      "  (Highest:number),\n",
      "  (Lowest:number),\n",
      "  (Average:number),\n",
      "]\n",
      "# Table: singer\n",
      "[\n",
      "  (Singer_ID:number),\n",
      "  (Name:number),\n",
      "  (Country:text),\n",
      "  (Song_Name:text),\n",
      "  (Song_release_year:text),\n",
      "  (Age:text),\n",
      "  (Is_male:number),\n",
      "]\n",
      "# Table: concert\n",
      "[\n",
      "  (concert_ID:others),\n",
      "  (concert_Name:number),\n",
      "  (Theme:text),\n",
      "  (Stadium_ID:text),\n",
      "  (Year:text),\n",
      "]\n",
      "# Table: singer_in_concert\n",
      "[\n",
      "  (concert_ID:text),\n",
      "  (Singer_ID:number),\n",
      "]\n",
      "\n",
      "【Foreign keys】\n",
      "concert.Stadium_ID=stadium.Stadium_ID\n",
      "singer_in_concert.Singer_ID=singer.Singer_ID\n",
      "singer_in_concert.concert_ID=concert.concert_ID\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = {\n",
    "    \"question\": \"How many singers do we have?\",\n",
    "    \"schema\": \"【DB_ID】 concert_singer\\n【Schema】\\n# Table: stadium\\n[\\n  (Stadium_ID:text),\\n  (Location:number),\\n  (Name:text),\\n  (Capacity:text),\\n  (Highest:number),\\n  (Lowest:number),\\n  (Average:number),\\n]\\n# Table: singer\\n[\\n  (Singer_ID:number),\\n  (Name:number),\\n  (Country:text),\\n  (Song_Name:text),\\n  (Song_release_year:text),\\n  (Age:text),\\n  (Is_male:number),\\n]\\n# Table: concert\\n[\\n  (concert_ID:others),\\n  (concert_Name:number),\\n  (Theme:text),\\n  (Stadium_ID:text),\\n  (Year:text),\\n]\\n# Table: singer_in_concert\\n[\\n  (concert_ID:text),\\n  (Singer_ID:number),\\n]\\n\\n【Foreign keys】\\nconcert.Stadium_ID=stadium.Stadium_ID\\nsinger_in_concert.Singer_ID=singer.Singer_ID\\nsinger_in_concert.concert_ID=concert.concert_ID\\n\",\n",
    "    \"sql\": \"SELECT count(*) FROM singer\",\n",
    "    \"db_id\": \"concert_singer\"\n",
    "  }\n",
    "\n",
    "print(t['schema'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
